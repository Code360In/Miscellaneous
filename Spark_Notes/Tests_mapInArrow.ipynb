{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c72076c",
   "metadata": {},
   "source": [
    "# Tests of PySpark UDF with mapInArrow vs. mapInPandas\n",
    "Spark 3.3.0 comes with an improvement in the Python UDF API: mapInArrow is a new feature in Spark 3.3.0, it works in a simmilar way to mapInPandas but it skips the pandas conversion steps.  \n",
    "This notebook uses the awkward array library to process the UDF instead of pandas,\n",
    "which has performance advantages for complex data types, in particular arrays.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download or install pyspark\n",
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57985ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/24 14:26:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# This is a new feature in Spark 3.3.0\n",
    "# See https://issues.apache.org/jira/browse/SPARK-37227\n",
    "\n",
    "# optionally use findspark\n",
    "# import findspark\n",
    "# findspark.init(\"/home/luca/Spark/spark-3.3.0-bin-hadoop3\")\n",
    "\n",
    "# use only 1 core to make performance comparisons easier/cleaner\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"dimuon mass\")  \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cfe47f",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9682c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple tests: create data from memory\n",
    "\n",
    "# We use a schema with a column that is a numerical array \n",
    "# this is where converting to Pandas is often slow\n",
    "\n",
    "df = spark.sql(\"select Array(rand(),rand(),rand()) col3 from range(1e8)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0325fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.22 ms, sys: 1.03 ms, total: 6.25 ms\n",
      "Wall time: 5.75 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# write to a noop sink for testing purposes\n",
    "# this is to test the speed of processing the dataframe with no additional operations\n",
    "\n",
    "df.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab6a52",
   "metadata": {},
   "source": [
    "## mapInPAndas tests\n",
    "These tests use mapInPandas, compare with the results when using mapInArrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d99dd",
   "metadata": {},
   "source": [
    "### Test 1 - dummy UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff98c7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 243 ms, sys: 29.8 ms, total: 273 ms\n",
      "Wall time: 42.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "  \n",
    "# A dummy UDF that just returns the input data\n",
    "def UDF_dummy(iterator):\n",
    "    for batch in iterator:\n",
    "        yield batch\n",
    "\n",
    "df.mapInPandas(UDF_dummy, df.schema).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dacbb4",
   "metadata": {},
   "source": [
    "### Test 2: square the array elements with mapInPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5edfeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 ms, sys: 1.98 ms, total: 20.5 ms\n",
      "Wall time: 1min 32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# UDF function that squares the input\n",
    "def UDF_pandas_square(iterator):\n",
    "    for batch in iterator:\n",
    "        yield batch*batch\n",
    "        \n",
    "df.mapInPandas(UDF_pandas_square, df.schema).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a86834b",
   "metadata": {},
   "source": [
    "## mapInArrow tests\n",
    "These tests use mapInArrow, compare with the results when using mapInPandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79665e77",
   "metadata": {},
   "source": [
    "### Test 3: dummy UDF using mapInArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23400f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 ms, sys: 3.46 ms, total: 15.7 ms\n",
      "Wall time: 22.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# A dummy UDF that just returns the input data\n",
    "def UDF_dummy(iterator):\n",
    "    for batch in iterator:\n",
    "        yield batch\n",
    "        \n",
    "df.mapInArrow(UDF_dummy, df.schema).write.format(\"noop\").mode(\"overwrite\").save() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361dfa3d",
   "metadata": {},
   "source": [
    "### Test 4: dummy UDF processing using mapInArrow and awkward array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the awkward arrays library\n",
    "# https://awkward-array.readthedocs.io/en/latest/\n",
    "\n",
    "! pip install awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7ab4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 170 ms, sys: 35.3 ms, total: 205 ms\n",
      "Wall time: 18.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import awkward as ak\n",
    "\n",
    "# a dummy UDF that convert back and forth to awkward arrays\n",
    "# it just returns the input data\n",
    "def UDF_dummy_with_awkward_array(iterator):\n",
    "    for batch in iterator:\n",
    "        b = ak.from_arrow(batch)\n",
    "        yield from ak.to_arrow_table(b).to_batches()\n",
    "\n",
    "df.mapInArrow(UDF_dummy_with_awkward_array, df.schema).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49b2aa",
   "metadata": {},
   "source": [
    "### Test 5: square the array elements using awkward array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ef09eb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.4 ms, sys: 5.14 ms, total: 11.5 ms\n",
      "Wall time: 22.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "\n",
    "def UDF_awkward_array_square(iterator):\n",
    "    for batch in iterator:\n",
    "        b = ak.from_arrow(batch)\n",
    "        b2 = ak.zip({\"col3\": np.square(b[\"col3\"])}, depth_limit=1)\n",
    "        yield from ak.to_arrow_table(b2).to_batches()\n",
    "\n",
    "df.mapInArrow(UDF_awkward_array_square, df.schema).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243325d0",
   "metadata": {},
   "source": [
    "## DataFrame API with higher-order functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7a052",
   "metadata": {},
   "source": [
    "### Test 6: Square the array elements using Spark higher order function for array processing\n",
    "If you can process arrays within the JVM, with Dataframe/SQL functions, is often advantageous compared to Python UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8815fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.49 ms, sys: 1.55 ms, total: 8.04 ms\n",
      "Wall time: 22.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df2 = df.selectExpr(\"transform(col3, x -> x * x) as col3_squared\")\n",
    "\n",
    "df2.write.format(\"noop\").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213361e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
